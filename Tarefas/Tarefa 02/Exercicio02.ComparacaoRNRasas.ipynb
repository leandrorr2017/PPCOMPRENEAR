{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais Artificiais - IFES - PPCOMP\n",
    "### Exercicio 02\n",
    "### Comparação de Redes Neurais Rasas em Múltiplos Datasets\n",
    "### Perceptron (Atividade 1), Perceptron SciKit, MLP (1 Hidden Layer), Linear SVM, SGD (Hinge Loss)\n",
    "### Datasets: Breast Cancer,  Dummy datasets (*)\n",
    "\n",
    "##### (*) Utilizada a implementação do PerformanceEvaluator desenvolvido na disciplina de Reconhecimento de Padrões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator,ClassifierMixin\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Classificadores\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do scikit-learn 0.21.2.\n"
     ]
    }
   ],
   "source": [
    "print('Versão do scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Binários\n",
    "dX_AllDatasets={}\n",
    "dy_AllDatasets={}\n",
    "\n",
    "# Breast Cancer\n",
    "data = load_breast_cancer()\n",
    "X,y = data.data,data.target\n",
    "dX_AllDatasets['breast_cancer']=X\n",
    "dy_AllDatasets['breast_cancer']=y\n",
    "\n",
    "# Dummy Dataset 1 - sklearn.datasets.make_classification\n",
    "# One informative feature, one cluster per class\n",
    "X, y = make_classification(n_samples=1000,n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "dX_AllDatasets['dummy_ds_1']=X\n",
    "dy_AllDatasets['dummy_ds_1']=y\n",
    "\n",
    "# Dummy Dataset 2 - sklearn.datasets.make_classification\n",
    "# Two informative features, one cluster per class\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1)\n",
    "dX_AllDatasets['dummy_ds_2']=X\n",
    "dy_AllDatasets['dummy_ds_2']=y\n",
    "\n",
    "# Dummy Dataset 3 - sklearn.datasets.make_classification\n",
    "# Two informative features, two clusters per class\n",
    "X, Y = make_classification(n_features=2, n_redundant=0, n_informative=2)\n",
    "dX_AllDatasets['dummy_ds_3']=X\n",
    "dy_AllDatasets['dummy_ds_3']=y\n",
    "\n",
    "# Dummy Dataset 4 - sklearn.datasets.make_classification\n",
    "# 10.000 Samples com 10% de \"ruído\"\n",
    "X, y = make_classification(\n",
    "    n_samples=10000, \n",
    "    n_features=25,\n",
    "    flip_y=0.1) \n",
    "dX_AllDatasets['dummy_ds_4_10000_10_noise']=X\n",
    "dy_AllDatasets['dummy_ds_4_10000_10_noise']=y\n",
    "\n",
    "# Dummy Dataset 5 - sklearn.datasets.make_classification\n",
    "# 10.000 Samples - Difícil separação\n",
    "X, y = make_classification(\n",
    "    n_samples=10000, \n",
    "    n_features=25,\n",
    "    class_sep=0.1) # class_sep padrão=1.0. Menor o valor, mais dificil a classificação\n",
    "dX_AllDatasets['dummy_ds_5_10000_hard_sep']=X\n",
    "dy_AllDatasets['dummy_ds_5_10000_hard_sep']=y\n",
    "\n",
    "\n",
    "# Dummy Dataset 6 - sklearn.datasets.make_classification\n",
    "# 5.000 Samples - Ajuste na contribuição das features\n",
    "X, y = make_classification(n_samples=5000, \n",
    "    n_features=25, \n",
    "    n_redundant=10, # 10 das 25 features serão combinações das outras\n",
    "    n_repeated=5) # e 5 das 25 serão duplicadas\n",
    "dX_AllDatasets['dummy_ds_6_5000_feat_contrib']=X\n",
    "dy_AllDatasets['dummy_ds_6_5000_feat_contrib']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronPPCOMPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):    \n",
    "        r = np.dot(X, self.w) + self.b\n",
    "        if np.isscalar(r):\n",
    "            if r>=0.0:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        else:            \n",
    "            for i in range(len(r)):\n",
    "                if r[i]>=0.0:\n",
    "                    r[i]=1.0\n",
    "                else:\n",
    "                    r[i]=0.0\n",
    "            return r\n",
    "            \n",
    "    def fit(self, X, y, e=100,learn_r=0.001):\n",
    "        # Inicializa pesos (w) e bias (b)\n",
    "        \n",
    "        # Inicializacao com Zeros (0)\n",
    "        #self.w = np.zeros((X.shape[1], )) # X.shape[1] = total de caracteristicas do dataset\n",
    "        #self.b = 0.0\n",
    "        \n",
    "        # Inicialização com valores aleatorios\n",
    "        #self.w = np.random.normal(size=X.shape[1])\n",
    "        self.w = np.random.random((X.shape[1], ))\n",
    "        self.b = np.random.random()\n",
    "        \n",
    "        for f in range(e):\n",
    "            error_conv = 0 # avaliar convergencia\n",
    "            for xi, yi in zip(X, y):\n",
    "                err = yi - self.predict(xi)\n",
    "                if err != 0:                    \n",
    "                    self.w += learn_r*err*xi # w <- w + α(y — f(x))x\n",
    "                    self.b += learn_r*err\n",
    "                    error_conv+=1\n",
    "            if error_conv == 0:\n",
    "                break\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluator():\n",
    "  def __init__(self, X, y,cv,scaler):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    "    self.cv=cv\n",
    "    self.scaler=scaler\n",
    "  def score(self, pipe):\n",
    "    scores=cross_val_score(pipe, self.X,self.y, cv=self.cv) # (Stratified)KFold\n",
    "    return scores \n",
    "  def evaluate(self, clfs):\n",
    "    best_overal=0\n",
    "    for name,clf in clfs:\n",
    "        if self.scaler==True:\n",
    "            pipe = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                   ('classifier', clf)])\n",
    "        else:\n",
    "            pipe = clf\n",
    "        t_inicio = time.time()\n",
    "        scores=self.score(pipe)\n",
    "        t_fim = time.time()\n",
    "        print('Mean: %0.7f Std: %0.7f(+/-) Best: %0.7f Time: %.2f(s) [%s]' % (scores.mean(), scores.std(), scores.max(),t_fim-t_inicio,name))\n",
    "        if (scores.mean()>best_overal):\n",
    "            best_overal=scores.mean()\n",
    "            best_pipe=pipe\n",
    "            best_clf_name=name    \n",
    "    print('Best Estimator: ',best_clf_name)        \n",
    "    ### Matriz de Confusão ilustrativa para o melhor estimator\n",
    "    X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.20)\n",
    "    best_pipe.fit(X_train,y_train)\n",
    "    y_p=best_pipe.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test,y_p)\n",
    "    print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparativo de Redes Neurais Rasas com multiplos datasets\n",
      "\n",
      "========================================\n",
      "breast_cancer\n",
      "----------------------------------------\n",
      "Mean: 0.8719969 Std: 0.0425969(+/-) Best: 0.9292035 Time: 0.90(s) [PerceptronPPCOMP]\n",
      "Mean: 0.8025702 Std: 0.1697021(+/-) Best: 0.9217391 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.9491343 Std: 0.0267773(+/-) Best: 0.9911504 Time: 0.17(s) [LinearSVM]\n",
      "Mean: 0.9068103 Std: 0.0300324(+/-) Best: 0.9292035 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.8290881 Std: 0.1041403(+/-) Best: 0.9217391 Time: 0.24(s) [MLP]\n",
      "Best Estimator:  LinearSVM\n",
      "[[31  3]\n",
      " [ 4 76]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_1\n",
      "----------------------------------------\n",
      "Mean: 0.9929648 Std: 0.0098472(+/-) Best: 1.0000000 Time: 1.39(s) [PerceptronPPCOMP]\n",
      "Mean: 0.9879698 Std: 0.0081538(+/-) Best: 1.0000000 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.9909748 Std: 0.0092091(+/-) Best: 1.0000000 Time: 0.01(s) [LinearSVM]\n",
      "Mean: 0.9959899 Std: 0.0058586(+/-) Best: 1.0000000 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.9799647 Std: 0.0170605(+/-) Best: 1.0000000 Time: 0.26(s) [MLP]\n",
      "Best Estimator:  SGD_LossHinge\n",
      "[[102   0]\n",
      " [  1  97]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_2\n",
      "----------------------------------------\n",
      "Mean: 0.9700000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.17(s) [PerceptronPPCOMP]\n",
      "Mean: 0.9800000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.00(s) [PerceptronSciKit]\n",
      "Mean: 0.9800000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.00(s) [LinearSVM]\n",
      "Mean: 0.9800000 Std: 0.0400000(+/-) Best: 1.0000000 Time: 0.00(s) [SGD_LossHinge]\n",
      "Mean: 0.7900000 Std: 0.2437212(+/-) Best: 1.0000000 Time: 0.07(s) [MLP]\n",
      "Best Estimator:  PerceptronSciKit\n",
      "[[10  0]\n",
      " [ 0 10]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_3\n",
      "----------------------------------------\n",
      "Mean: 0.5400000 Std: 0.1019804(+/-) Best: 0.7000000 Time: 0.22(s) [PerceptronPPCOMP]\n",
      "Mean: 0.5200000 Std: 0.0812404(+/-) Best: 0.6000000 Time: 0.00(s) [PerceptronSciKit]\n",
      "Mean: 0.4300000 Std: 0.0871780(+/-) Best: 0.5500000 Time: 0.01(s) [LinearSVM]\n",
      "Mean: 0.3800000 Std: 0.0748331(+/-) Best: 0.4500000 Time: 0.00(s) [SGD_LossHinge]\n",
      "Mean: 0.4700000 Std: 0.0927362(+/-) Best: 0.5500000 Time: 0.06(s) [MLP]\n",
      "Best Estimator:  PerceptronPPCOMP\n",
      "[[ 1 10]\n",
      " [ 1  8]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_4_10000_10_noise\n",
      "----------------------------------------\n",
      "Mean: 0.7925000 Std: 0.0288184(+/-) Best: 0.8300000 Time: 17.09(s) [PerceptronPPCOMP]\n",
      "Mean: 0.7817000 Std: 0.0326046(+/-) Best: 0.8280000 Time: 0.03(s) [PerceptronSciKit]\n",
      "Mean: 0.8917000 Std: 0.0051049(+/-) Best: 0.8995000 Time: 3.53(s) [LinearSVM]\n",
      "Mean: 0.8840000 Std: 0.0027203(+/-) Best: 0.8885000 Time: 0.18(s) [SGD_LossHinge]\n",
      "Mean: 0.8883000 Std: 0.0031401(+/-) Best: 0.8935000 Time: 2.02(s) [MLP]\n",
      "Best Estimator:  LinearSVM\n",
      "[[925 104]\n",
      " [116 855]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_5_10000_hard_sep\n",
      "----------------------------------------\n",
      "Mean: 0.5156000 Std: 0.0173764(+/-) Best: 0.5400000 Time: 21.60(s) [PerceptronPPCOMP]\n",
      "Mean: 0.5084000 Std: 0.0030887(+/-) Best: 0.5135000 Time: 0.04(s) [PerceptronSciKit]\n",
      "Mean: 0.5603000 Std: 0.0074606(+/-) Best: 0.5720000 Time: 9.11(s) [LinearSVM]\n",
      "Mean: 0.5246000 Std: 0.0060778(+/-) Best: 0.5330000 Time: 0.20(s) [SGD_LossHinge]\n",
      "Mean: 0.6095000 Std: 0.0046260(+/-) Best: 0.6155000 Time: 4.50(s) [MLP]\n",
      "Best Estimator:  MLP\n",
      "[[540 453]\n",
      " [354 653]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_6_5000_feat_contrib\n",
      "----------------------------------------\n",
      "Mean: 0.9123974 Std: 0.0127032(+/-) Best: 0.9310689 Time: 8.19(s) [PerceptronPPCOMP]\n",
      "Mean: 0.8923936 Std: 0.0120148(+/-) Best: 0.9110889 Time: 0.02(s) [PerceptronSciKit]\n",
      "Mean: 0.9369968 Std: 0.0087616(+/-) Best: 0.9480519 Time: 0.50(s) [LinearSVM]\n",
      "Mean: 0.9275944 Std: 0.0146409(+/-) Best: 0.9440000 Time: 0.08(s) [SGD_LossHinge]\n",
      "Mean: 0.9455982 Std: 0.0048626(+/-) Best: 0.9520000 Time: 1.66(s) [MLP]\n",
      "Best Estimator:  MLP\n",
      "[[520   7]\n",
      " [ 55 418]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Comparativo de Redes Neurais Rasas com multiplos datasets\")\n",
    "\n",
    "# Classificadores de interesse com respectivos hyper-parametros\n",
    "clfs = [\n",
    "    ('PerceptronPPCOMP',PerceptronPPCOMPClassifier()),\n",
    "    ('PerceptronSciKit',Perceptron(tol=1e-3, random_state=0)),\n",
    "    ('LinearSVM',SVC(kernel=\"linear\", C=0.025)),\n",
    "    ('SGD_LossHinge',SGDClassifier(loss='hinge',max_iter=1000, tol=1e-3)),\n",
    "    ('MLP',MLPClassifier(max_iter=500,early_stopping=True,hidden_layer_sizes=(100,)))\n",
    "]\n",
    "\n",
    "### Parametros complementaras ###\n",
    "# cross-validation folds\n",
    "cv = 5\n",
    "# habilita ou nao scaler (standard scaler)\n",
    "scaler = False\n",
    "#################################\n",
    "\n",
    "for key in dX_AllDatasets.keys():\n",
    "    print(\"\\n\" +\"=\"*40)\n",
    "    print(key)\n",
    "    print(\"-\"*40)    \n",
    "    X,y=dX_AllDatasets[key],dy_AllDatasets[key]\n",
    "    pe = PerformanceEvaluator(X,y,cv,scaler)\n",
    "    pe.evaluate(clfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "breast_cancer\n",
      "----------------------------------------\n",
      "Mean: 0.9701578 Std: 0.0068991(+/-) Best: 0.9823009 Time: 0.88(s) [PerceptronPPCOMP]\n",
      "Mean: 0.9666795 Std: 0.0148693(+/-) Best: 0.9823009 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.9718969 Std: 0.0065201(+/-) Best: 0.9823009 Time: 0.02(s) [LinearSVM]\n",
      "Mean: 0.9718969 Std: 0.0128707(+/-) Best: 0.9826087 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.9279877 Std: 0.0231880(+/-) Best: 0.9734513 Time: 0.19(s) [MLP]\n",
      "Best Estimator:  LinearSVM\n",
      "[[42  5]\n",
      " [ 0 67]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_1\n",
      "----------------------------------------\n",
      "Mean: 0.9899797 Std: 0.0095189(+/-) Best: 1.0000000 Time: 1.50(s) [PerceptronPPCOMP]\n",
      "Mean: 0.9899797 Std: 0.0071138(+/-) Best: 1.0000000 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.9909748 Std: 0.0092091(+/-) Best: 1.0000000 Time: 0.02(s) [LinearSVM]\n",
      "Mean: 0.9949749 Std: 0.0077849(+/-) Best: 1.0000000 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.9639341 Std: 0.0248987(+/-) Best: 0.9950000 Time: 0.22(s) [MLP]\n",
      "Best Estimator:  SGD_LossHinge\n",
      "[[ 93   0]\n",
      " [  0 107]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_2\n",
      "----------------------------------------\n",
      "Mean: 0.9800000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.17(s) [PerceptronPPCOMP]\n",
      "Mean: 0.9800000 Std: 0.0400000(+/-) Best: 1.0000000 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.9800000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.01(s) [LinearSVM]\n",
      "Mean: 0.9800000 Std: 0.0244949(+/-) Best: 1.0000000 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.8800000 Std: 0.0927362(+/-) Best: 1.0000000 Time: 0.08(s) [MLP]\n",
      "Best Estimator:  PerceptronPPCOMP\n",
      "[[ 9  0]\n",
      " [ 1 10]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_3\n",
      "----------------------------------------\n",
      "Mean: 0.4900000 Std: 0.1019804(+/-) Best: 0.6000000 Time: 0.22(s) [PerceptronPPCOMP]\n",
      "Mean: 0.4600000 Std: 0.0734847(+/-) Best: 0.5500000 Time: 0.01(s) [PerceptronSciKit]\n",
      "Mean: 0.4300000 Std: 0.1166190(+/-) Best: 0.6000000 Time: 0.01(s) [LinearSVM]\n",
      "Mean: 0.5500000 Std: 0.0894427(+/-) Best: 0.7000000 Time: 0.01(s) [SGD_LossHinge]\n",
      "Mean: 0.5100000 Std: 0.0734847(+/-) Best: 0.6500000 Time: 0.05(s) [MLP]\n",
      "Best Estimator:  SGD_LossHinge\n",
      "[[5 6]\n",
      " [3 6]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_4_10000_10_noise\n",
      "----------------------------------------\n",
      "Mean: 0.7917000 Std: 0.0234128(+/-) Best: 0.8235000 Time: 16.83(s) [PerceptronPPCOMP]\n",
      "Mean: 0.7792000 Std: 0.0399757(+/-) Best: 0.8315000 Time: 0.07(s) [PerceptronSciKit]\n",
      "Mean: 0.8913000 Std: 0.0052115(+/-) Best: 0.8990000 Time: 3.88(s) [LinearSVM]\n",
      "Mean: 0.8819000 Std: 0.0013928(+/-) Best: 0.8835000 Time: 0.21(s) [SGD_LossHinge]\n",
      "Mean: 0.8893000 Std: 0.0024207(+/-) Best: 0.8940000 Time: 2.18(s) [MLP]\n",
      "Best Estimator:  LinearSVM\n",
      "[[899 103]\n",
      " [122 876]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_5_10000_hard_sep\n",
      "----------------------------------------\n",
      "Mean: 0.5203000 Std: 0.0133214(+/-) Best: 0.5365000 Time: 21.95(s) [PerceptronPPCOMP]\n",
      "Mean: 0.5066000 Std: 0.0221120(+/-) Best: 0.5350000 Time: 0.08(s) [PerceptronSciKit]\n",
      "Mean: 0.5632000 Std: 0.0084356(+/-) Best: 0.5765000 Time: 9.56(s) [LinearSVM]\n",
      "Mean: 0.5200000 Std: 0.0137186(+/-) Best: 0.5395000 Time: 0.24(s) [SGD_LossHinge]\n",
      "Mean: 0.6138000 Std: 0.0130752(+/-) Best: 0.6350000 Time: 3.33(s) [MLP]\n",
      "Best Estimator:  MLP\n",
      "[[491 530]\n",
      " [287 692]]\n",
      "\n",
      "========================================\n",
      "dummy_ds_6_5000_feat_contrib\n",
      "----------------------------------------\n",
      "Mean: 0.9065958 Std: 0.0191927(+/-) Best: 0.9330669 Time: 8.17(s) [PerceptronPPCOMP]\n",
      "Mean: 0.8513896 Std: 0.0731813(+/-) Best: 0.9310689 Time: 0.03(s) [PerceptronSciKit]\n",
      "Mean: 0.9359968 Std: 0.0086710(+/-) Best: 0.9470000 Time: 0.52(s) [LinearSVM]\n",
      "Mean: 0.9287934 Std: 0.0140301(+/-) Best: 0.9470000 Time: 0.10(s) [SGD_LossHinge]\n",
      "Mean: 0.9405946 Std: 0.0134164(+/-) Best: 0.9580420 Time: 1.62(s) [MLP]\n",
      "Best Estimator:  MLP\n",
      "[[482   9]\n",
      " [ 37 472]]\n"
     ]
    }
   ],
   "source": [
    "### Ativando StandardScaler ###\n",
    "scaler = True\n",
    "#################################\n",
    "\n",
    "for key in dX_AllDatasets.keys():\n",
    "    print(\"\\n\" +\"=\"*40)\n",
    "    print(key)\n",
    "    print(\"-\"*40)    \n",
    "    X,y=dX_AllDatasets[key],dy_AllDatasets[key]\n",
    "    pe = PerformanceEvaluator(X,y,cv,scaler)\n",
    "    pe.evaluate(clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observaçoes sobre o experimento:\n",
    "\n",
    "* Para alguns datasets (Ex: Breast Cancer) a normalização dos dados trouxe uma melhoria significativa.\n",
    "* Notória a vantagem da MLP (1 hidden layer) contra os classificadores lineares em um cenário de difício separação (Dummy Dataset 5)\n",
    "\n",
    "##### SGD (Hinge Loss) x Linear SVM\n",
    "Pelo meu entendimento o SGDClassifer é um otimizador para classificadores lineares utilizando o SGD. Por padrão ele otimiza uma SVM Linear com a função de custo Hinge. Com o uso de uma função de custo do tipo log, por exemplo, otimizaria uma regressão logística. Outro ponto é que o mesmo faz uso de mini-batches. \n",
    "\n",
    ">This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method...The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).\n",
    "\n",
    "\n",
    "Interessante observar que no experimento o SGD não obteve sucesso na otimização em muitos casos com os parametros escolhidos. O tunning destes parâmetros não foi objeto de análise pelo menos nesta primeira experimentação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
